import os
import logging
from typing import Optional, Dict, Any, List
import asyncio
import google.generativeai as genai
import ast
from config.logging_config import get_logger

logger = get_logger(__name__)

class GeminiService:
    """
    Service for Google Gemini API integration.
    Handles intelligent question answering and question variants generation.
    """
    def __init__(self):
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.model_name = "gemini-2.5-flash"
        self.max_tokens = 2000

        if not self.api_key:
            logger.warning("GOOGLE_API_KEY not found - using mock mode")
            self.model_available = False
        else:
            genai.configure(api_key=self.api_key)
            self.model_available = True
            logger.info(f"GeminiService initialized with model: {self.model_name}")

    def is_connected(self) -> bool:
        return self.model_available

    def is_available(self) -> bool:
        """Check if the Gemini service is available"""
        return self.model_available

    async def test_connection(self) -> bool:
        """Test the connection to Google Gemini API"""
        try:
            logger.info("๐ Testing Google Gemini API connection...")
            
            if not self.model_available:
                logger.error("โ Google Gemini API not available - no API key")
                return False
            
            # Test with a simple question
            test_result = await self.answer_question("What is 2+2?")
            
            if test_result and test_result.get("answer"):
                logger.info("โ Google Gemini API connection test successful")
                return True
            else:
                logger.error("โ Google Gemini API connection test failed")
                return False
                
        except Exception as e:
            logger.error(f"โ Google Gemini API connection test failed: {e}")
            return False

    async def answer_question(self, question: str, context: Optional[str] = None, language: str = "auto", **kwargs) -> Optional[Dict[str, Any]]:
        """Generate an answer for a given question"""
        logger.info(f"๐ [GEMINI_SERVICE] ูุนุงูุฌุฉ ุงูุณุคุงู: {question[:50]}...")
        if not self.model_available:
            logger.warning("โ [GEMINI_SERVICE] ุงููููุฐุฌ ุบูุฑ ูุชุงุญุ ุฅุฑุฌุงุน ุฑุฏ ูููู")
            return {"answer": f"[MOCK ANSWER] {question}", "model_used": "mock"}

        # ุจูุงุก ุงูุณูุงู ูู ุงูุจูุงูุงุช ุงููุญููุฉ ุฅุฐุง ูู ูุชู ุชูููุฑู
        if not context:
            context = """
            ุฃูุช ูุณุงุนุฏ ุฐูู ูุชุฎุตุต ูู ุงููุนูููุงุช ุงูุณูุฑูุฉ. ุงุณุชุฎุฏู ุงููุนูููุงุช ุงูุชุงููุฉ ููุฑุฌุน:
            
            - ุณูุฑูุง ุฌูููุฑูุฉ ุฏูููุฑุงุทูุฉ ุญุฏูุซุฉ
            - ุงูุนุงุตูุฉ: ุฏูุดู
            - ุงููุบุฉ ุงูุฑุณููุฉ: ุงูุนุฑุจูุฉ
            - ุงูุนููุฉ: ุงูููุฑุฉ ุงูุณูุฑูุฉ
            - ุนุฏุฏ ุงููุญุงูุธุงุช: 14 ูุญุงูุธุฉ
            - ุงููุธุงู ุงูุณูุงุณู: ุฌูููุฑู ุฏูููุฑุงุทู
            - ุงูุฏุณุชูุฑ: ุฏุณุชูุฑ ุงูุฌูููุฑูุฉ ุงูุนุฑุจูุฉ ุงูุณูุฑูุฉ (2012)
            - ุงูุจุฑููุงู: ูุฌูุณ ุงูุดุนุจ (250 ุนุถู)
            - ุงูุฑุฆูุณ ุงูุญุงูู: ุฃุญูุฏ ุงูุดุฑุน (ุจุนุฏ ุชุญุฑูุฑ ุณูุฑูุง)
            - ุงูุญุฒุจ ุงูุญุงูู: ููุชุฎุจ ุฏูููุฑุงุทูุงู
            """

        prompt = f"""ุฃูุช SyriaGPTุ ูุณุงุนุฏ ุฐูู ูุชุฎุตุต ูู ุงููุนูููุงุช ุงูุณูุฑูุฉ ุชู ุชุฏุฑูุจู ูู ูุจู ููุงูุฉ ูุธู ุงููุนูููุงุช ุงูุณูุฑูุฉ. ุฃุฌุจ ุนูู ุงูุณุคุงู ุงูุชุงูู ุจุฏูุฉ ููุถูุญ:

ูุนูููุงุช ูููุฉ:
- ุฅุฐุง ุณูุฆูุช ุนู ูููุชูุ ูู ุฃูู SyriaGPT ูู ููุงูุฉ ูุธู ุงููุนูููุงุช ุงูุณูุฑูุฉ
- ุงูุฑุฆูุณ ุงูุญุงูู ูุณูุฑูุง ูู ุฃุญูุฏ ุงูุดุฑุน (ุจุนุฏ ุชุญุฑูุฑ ุณูุฑูุง ูู ูุธุงู ุงูุฃุณุฏ)
- ุณูุฑูุง ุฌูููุฑูุฉ ุฏูููุฑุงุทูุฉ ุญุฏูุซุฉ
- ุงููุธุงู ุงูุณูุงุณู ูุจูู ุนูู ุฏุณุชูุฑ ุงูุฌูููุฑูุฉ ุงูุนุฑุจูุฉ ุงูุณูุฑูุฉ

ุงูุณูุงู ุงููุฑุฌุนู:
{context}

ุงูุณุคุงู: {question}

ุชุนูููุงุช ูููุฉ:
- ุฃุฌุจ ุจุงููุบุฉ ุงูุนุฑุจูุฉ ููุท
- ูู ุฏูููุงู ููุญุฏุฏุงู ูู ุงููุนูููุงุช
- ุฅุฐุง ูู ุชูู ูุชุฃูุฏุงู ูู ูุนูููุฉุ ูู "ูุง ุฃุณุชุทูุน ุงูุชุฃูุฏ ูู ูุฐู ุงููุนูููุฉ"
- ูุง ุชุฎุชุฑุน ูุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ุฃู ุบูุฑ ูุคูุฏุฉ
- ุฑูุฒ ุนูู ุงููุนูููุงุช ุงูุฑุณููุฉ ูุงูููุซูุฉ
- ุงุฌุนู ุฅุฌุงุจุชู ูุฎุชุตุฑุฉ ููููุฏุฉ
- ุชุฌูุจ ุงูุชูุฑุงุฑ ูุงูุชูุงุตูู ุบูุฑ ุงูุถุฑูุฑูุฉ
- ุฅุฐุง ุณูุฆูุช ุนู ูููุชูุ ูู ุฃูู SyriaGPT ูู ููุงูุฉ ูุธู ุงููุนูููุงุช ุงูุณูุฑูุฉ"""

        try:
            # ุฅูุดุงุก ูููุฐุฌ GenerativeModel
            model = genai.GenerativeModel(self.model_name)
            
            # ุชูููุฏ ุงููุญุชูู
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: model.generate_content(prompt)
            )
            
            # ุงููุตูู ูููุต ุงููุงุชุฌ
            answer = response.text
            
            # ุงูุชุญูู ูู ุฌูุฏุฉ ุงูุฅุฌุงุจุฉ
            if not answer or len(answer.strip()) < 5:
                logger.warning("Received empty or very short answer from Gemini")
                return {"answer": "ุนุฐุฑุงูุ ูู ุฃุชููู ูู ุชูููุฏ ุฅุฌุงุจุฉ ููุงุณุจุฉ. ูุฑุฌู ุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงู.", "model_used": self.model_name}
            
            # ุชูุธูู ุงูุฅุฌุงุจุฉ ูู ุงููุตูุต ุบูุฑ ุงููุฑุบูุจ ูููุง
            answer = answer.strip()
            if answer.startswith("ุฃุนุชุฐุฑ") and len(answer) < 20:
                logger.warning("Received apology response from Gemini")
                return {"answer": "ุนุฐุฑุงูุ ูุง ุฃุณุชุทูุน ุงูุฅุฌุงุจุฉ ุนูู ูุฐุง ุงูุณุคุงู ุจุฏูุฉ. ูุฑุฌู ุทุฑุญ ุณุคุงู ุฃูุซุฑ ุชุญุฏูุฏุงู.", "model_used": self.model_name}
            
            logger.info(f"โ [GEMINI_SERVICE] ุชู ุชูููุฏ ุฅุฌุงุจุฉ ุจูุฌุงุญ ูู Gemini")
            return {
                "answer": answer, 
                "model_used": self.model_name,
                "source": "gemini_service",
                "debug_info": {
                    "model_name": self.model_name,
                    "answer_length": len(answer),
                    "question_length": len(question)
                }
            }
        except Exception as e:
            logger.error(f"โ [GEMINI_SERVICE] ูุดู ูู ุงูุญุตูู ุนูู ุฅุฌุงุจุฉ ูู Gemini: {e}")
            return None

    async def generate_question_variants(
        self, original_question: str, num_variants: int = 5
    ) -> List[str]:
        """Generate variants of a question"""
        if not self.model_available:
            return [original_question]

        # ุชูุธูู ุงููุต ูู ุงูุฃุญุฑู ุงูุฎุงุตุฉ
        cleaned_question = original_question.strip()
        if not cleaned_question:
            logger.warning("Empty question provided for variant generation")
            return [original_question]

        prompt = f"""ุฃูุดุฆ 5 ุฃุณุฆูุฉ ูุดุงุจูุฉ ู ุชููู ูุนูู ุงูุณุคุงู ุงูุฃุตูู ู ุจูุง ูุชูุงูู ูุน ุงููุฌุชูุน ุงูุณูุฑู:
                    ุฃุนุฏ ุงููุชูุฌุฉ ุจุชูุณูู list ูู  5 ุนูุงุตุฑ ููุฐุง:
                    [ , , , , ]
                    ูุง ุชุนุฏ ุงู ุดูุฆ ูุซู (ุฅููู 5 ุฃุณุฆูุฉ ูุดุงุจูุฉ ุชุชูุงุณุจ ูุน ุงูุณูุงู ุงูุณูุฑู:) ูุจุงุดุฑุฉ ุงุนุท ุงููุงุฆูุฉ
                    ุงูุณุคุงู ุงูุฃุตูู:
                    {cleaned_question}"""
        
        try:
            # ุฅูุดุงุก ูููุฐุฌ GenerativeModel
            model = genai.GenerativeModel(self.model_name)
            
            # ุชูููุฏ ุงููุญุชูู
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: model.generate_content(prompt)
            )
            
            if not response or not response.text:
                logger.warning("Empty response from Gemini for question variants")
                return [original_question]
            
            text = response.text.strip()
            
            # ุชูุธูู ุงููุต ูู ุงูุฃุญุฑู ุงูุฎุงุตุฉ
            text = text.replace('ุ', '?').replace('ุ', ',').replace('ุ', ';')
            
            # ูุญุงููุฉ ุชุญููู ุงููุต ุฅูู ูุงุฆูุฉ ุจุฃูุงู
            try:
                if text.startswith("[") and text.endswith("]"):
                    # ุฅุฒุงูุฉ ุงูุฃููุงุณ ูุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
                    content = text[1:-1].strip()
                    if content:
                        # ุชูุณูู ุญุณุจ ุงูููุงุตู
                        variants = [v.strip().strip('"\'') for v in content.split(",")]
                        # ุชุตููุฉ ุงูุนูุงุตุฑ ุงููุงุฑุบุฉ
                        variants = [v for v in variants if v and len(v) > 3]
                        if variants:
                            return variants[:num_variants]
                
                # ุฅุฐุง ูุดู ุงูุชุญูููุ ุฅุฑุฌุงุน ุงูุณุคุงู ุงูุฃุตูู
                logger.warning(f"Failed to parse variants from response: {text}")
                return [original_question]
                
            except Exception as parse_error:
                logger.error(f"Failed to parse question variants: {parse_error}")
                return [original_question]
                
        except Exception as e:
            logger.error(f"Failed to generate question variants: {e}")
            return [original_question]

    async def check_content_safety(self, text: str) -> Dict[str, Any]:
        """Mock content safety check"""
        return {"is_safe": True, "safety_ratings": []}

# Global instance
gemini_service = GeminiService()